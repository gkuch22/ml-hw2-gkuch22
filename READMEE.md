# Machine Learning HW2 - gkuch22

  

## პროექტის სათაური:

**IEEE-CIS Fraud Detection**

კეგლის ამ competition -ში გვაქვს მოცემული შესრულებული ტრანზაქციების დიდი დატა, ასევე დამხმარე თებილი რომელშიც მოცემულია ზოგიერთი ტრანზაქციის შესახებ დამატებითი ინფრომაციები, ჩვენი მიზანია პატერნების სწორი დაჭერით მივხვდეთ მომავალში შესრულებული ახალი ტრანზაქციებიდან რომელია Fraud და რომელი არა.

რაც შეეხება მიდგომას, თუ როგორ გადავჭრათ ეს პრობლემა, საჭიროა გავანაალიზოთ დატა სწორად, ორივე თეიბლი ერთმანეთთან დავაკავშიროთ ლოგიკურა და აღმოვაჩინოთ ისეთი მახასიათებლები რომლებიც მომავალში საშუალებას მოგვცემს სწორად შევაფასოთ ტრანზაქციის ლეგალურობა/წესიერება. კონკრეტულ მიდგომებზე მოდელების დატრენინგების პარალელურად ვისაუბრებ.

  
  

## რეპოზიტორიის სტრუქტურა:

**hw2-begin** - საწყისი ელემენტარული ფაილი სადაც რამდენიმე data inspection-ის ნაწილი ჩანს, რომელმაც მიბიძგა პირველი მოდელის დატრენინგებისკენ. დანარჩენი data inspection-ის ნაწილები სხვა ფაილებშივეა მოცემული.

**hw2-model-experiment-{model_name}** - ამ ტიპის ფაილებში მოცემულია კლასიფიკაციის ამოცანაზე მორგებული კონკრეტული მოდელების იმპლემენტაცია.

**hw2-model-inference** - ამ ფაილში ხდება ყველა მოდელს შორის საუკეთესოს mlflow -დან ჩამოტვირთვა და საბოლოო submission-ის კეგლის competition-სთვის დაგენერირება.

**READMEE.md** - პროცესის დეტალური აღწერა.

  
  

## Feature Engineering:

პაიპლაინებით მარტივად და უფრო სუფთად სამუშაოდ შევქმენი რამდენიმე custom კლასი.

**CustomMajorNanHandlerClass** - ვინაიდან დატაში ხშირად გვხვდება ისეთი feature ები რომლებშიც საკმაოდ ბევრია Nan მნიშვნელობიანი row, გადავწყიტე შემომეღო ეს კლასი, რომელსაც გადაეცემა threshold კონსტრუქტორში და დატადან ამოყრის ისეთ სვეტებს რომლებშიც ნანების წილი ამ threshold ზე მეტია. ეს იდეა დაიბადა იქიდან რომ ვცდილობთ მოდელს შევუმსუბუქოთ მიწოდებული დატა, ისეთი სვეტები კი რომლებშიც მაგალითად 90% მეტი ნანები წერია თითქმის არაფრის მთქმელია საბოლოოდ ტარგეტზე. ეს კლასი წესით feature selection ის ნაწილში რომ ჩამეწერა ესეც სწორი იქნებოდა.

  

**Imputer** - წინა დავალებაში ამისთვისაც მქონდა შექმნილი კლასი, თუმცა ცალკე კლასად გატანას აზრი დიდად არ ჰქონდა რადგან ვრაპერივით იყო რომელსაც mean ან median ან zero ს გადავცემდი და მაინც SimpleImputer კლასს იყენებდა იმპლემტნაციაში. ზოგადად imputer ის არსებობა საკმაოდ საჭიროა რადგან ის სვეტები რომლებიც ნან ჰენდლერ კლასს გადაურჩა, სულ რომ გადავყაროთ არასწორია, რადგან გარკვეული ინფრომაციის მატარებელი მაინცაა ტარგეტზე. ამიტომ ნანების შევსება საშუალოთი ძირითადად უკეთესი ვარიანტია ხოლმე მაგრამ მაინც კონკრეტულად mean, median, zero თუ სხვა რამეს გამოვიყენებთ კონკრეტულ შემთხვევაზეა დამოკიდებული და მოდელის ტრენინგის პროცესში გამოჩნდება.

  

**CustomCategoricalEncoderClass** - კატეგორიული ცვლადების რიცხვითში გადაყვანა. კარგი იქნებოდა რომ მთლიანად OneHot ენკოდერის გამოყენება შეგვეძლოს კატეგორიული ცვლადების რიცხვითში გადასაყვანად, თუმცა ეს მონაცემებს ძალიან ტვრითავს რადგან თითო სვეტში 5 განსხავავებული მნიშვნელობა მაინც გვხვდება ალბათ. ამიტო გადავწყვიტე ასეთი კლასის შემოღება რომელსაც გადაეცემა threshold და ამასთანავე ორი ენქოდერ კლასი. პირველი კლასი ისეთ სვეტებს შეცვლის რომელშიც უნიკალური მნიშვნელობების რაოდენობა მოცემულ threshold ზე ნაკლებია, და მეორე შესაბამისად იმათ რომლებშიც მეტია. default-ად არჩეული მაქ ქვედა რეინჯში OneHot ხოლო ზედა რეინჯში Ordinal ენკოდერი. ხოლო threshold ის სწორად არჩევა უკვე ტრენინგის ნაწილშია წარმოდგენილი.

  

**CustomWoeEncoderClass** - კატეგორიული ცვლადების გადასაქცევად WOE ენკოდერიც შევქმენი, იდეა იგივეა, რაც წინა კლასი, თუმცა რომელი აჯობებს კონკრეტულ შემთხვევაზე და კონკრეტული მოდელის ტრენინგის პროცესზეა დამოკიდებული და ამიტომ შემდეგ ვისაუბრებ.

  

**Scaler** - imputer ის მსგავსად ამის კლასიც მქონდა წინა დავალებაში თუმცა ესეც ვრაპერივით იყო და გადაწყვიტე არ გამომეყინება აქ როგორც ცალკე კლასი, თუმცა თვითონ სქეილინგის იდეა საკმაოდ მნიშვნელოვანია დატას დამუშავებისას და საკმაოდ აუმჯობესებს ხოლმე მოდელს, რომ უფრო მაღალ მნიშვნელობიან თუმცა რეალურად უფრო ნაკლებად მნიშვნელოვან ცვლადებს არ მიაქციუოს ყურადღება მოდელმა სწავლის პროცესში და სწორ feature ებზე გააკეთოს აქცენტი. აქაც standard-ს გამოივყენებ minmax-ს თუ სხვა რამეს მომვალში გამოჩნდება.

  
  

## Feature Selection:

**CustomNuniqueHandlerClass** - დაახლოებით nan ჰენდლერის იდეის მატარებელია ეს კლასიც. მხოლოდ ამ შემთხვევაში ნან ველიუების მაგივრად ყრის ისეთ სვეტებს რომლებიც დიდი ოდენობით ერთი და იგივე მნიშვნელობას შეიცავენ. ამ კლასსაც გადეცემა threshold და წინა კლასის მსგავსად ამ threshold ზე მეტი წილის მქონე სვეტებს სადაც ერთი და იგივე მნიშვნელობები წერია გადაყრის. ამ კლასის შექმნის იდეაც იგივე მოტივით გაჩნდა რომ დატა შევუმსუბუქოთ მოდელს და არასაჭირო სვეტები გადავყაროთ, ასეთი ფიჩერები კი ნაკლებად მნიშვნელოვნად ითვლება რადგან მაგალითად სვეტი სადაც 85% სულ ერთნაირი მნიშვნელობები წერია ბევრის მთქმელი არაა ტარგეტზე.

  

**CustomCorrHandlerClass** - ცვლადების კორელაციას აკვირდება ტარგეტთან მიმართებით და ისეთ წყვილებს სადაც ორივეს მსგავასი გავლენა აქვს პრედიქშენზე თითო-თითო ცვლადს გადაუყრის. ამ მხრივ ნაწილი feature ების სუფთავდება, ცოტავდება და მოდელს საშუალება ეძლევა უკეთ კონცეტრირდეს საჭირო სვეტებზე.

  

**CustomSHAPSelectorClass** - შაპის feature importance-ის მიხედვით აბრუნებს მხოლოდ საჭირო ცვლადებს და დანარჩენებს გადაყრის, რომ მოდელმა სწორ ცვლადებზე გაამახვილოს ყურადღება.

  
  

## Training:

(Logisitc Regression-ის ტრენინგისას train და valid დატასეტების შედეგებს არ ვლოგავდი თურმე და პირიქით test-ის შედეგები მაქვს ატვირტული, ტრენინგის ნაწილში როგორც წესია train და valid ზე მიღებული შედეგებით მივდიოდი საუკეთესო მოდელისკენ, თუმცა ლოგგინგის კოდში მქონია არასწორად კოდი. სხვა მოდელებში გასწორებული მაქვს უკვე).

  
  

## **Logistic Regression**

დატას დამუშავებამდე, თავიდან მინდოდა უბრალოდ მეცადა ყველაზე მარტივი მოდელის, Logistic Regression-ის, დატრენინგება პირდაპირ Transaction თეიბლზე. მხოლოდ Nan მნიშვნელობები და კატეგორიული ცვლადები დავჰენდლე. accuracy 0.96 აჩვენა თუმცა სხვა მეტრიკებზე დაკვირვებით მარტივად მივხვდებით რომ ეს კარგი მაჩვენებელი არაა, დანარჩენი მნიშნველობების ცუდი ველიუები მიუთითებს რომ დატა დაუბალანსირებელია და მოდელის დატრენინგებამდე ეს მოსაგვარებელია. recall 0.000235 მოდელი თითქმის ვერ იჭერს fraud ებს. მოდელი underfit შია წასული რადგან თითქმის ვერ იჭერს rare შემთხვევებს რომლებიც ჩვენთვის უფრო მნიშვნელოვანია და ვერ განასხვავებს საჭირო და არასაჭირო ცვლადებს ერთამენითსგან.

  

შემდეგ გაშვებაზე გადავწყვიტე Transaction და Identity თებილები შემეერთებინა, რომ მენახა ზოგადად რა გავლენა ექნებოდა identity თეიბლს prediction ებზე. დასაწყისისთვის პირადაპირ TransactionID-ს გამოყენებით დავაჯოინე დატაფრეიმები, და შემდეგ ნან ველიუები საშუალოთი შევავსე ისევ რომ გამოტვებული დატაპოინტების ნანები გამქრალიყო. ასევე დავამატე NuniqueHanlder. ამან წინასთან შედარებით უკეთესი შედეგი მომცა, თუმცა ძალიან მცირედით. ეს მოსალოდნელიც იყო რადგან identity თეიბლი ტრანაქციების თეიბლის 25% ს შეიცავს მარტო, დანარჩენი როუების საშუალოთი შევსება დიდ ნოიზს დაამატებდა წესით.

  

ამის შემდეგ Transaction და Identity თებილების გადაბმასთან ერთად შემოვიღე key_id feature რომელიც card1, addr1 და P_emaildomain -ის გაერთიანებაა, ამის იქით სხვა წყვილების გადარჩევაც კარგი იქნებოდა მეტი დროითი რესურსის არსებობის შემთხვევაში. იმ პრობლემის მოსაგვარებლად რაც ლექციაზეც რამდენჯერმე განვიხილეთ, ასეთი ცვლადის შემოღება გადავწყვიტე, რომ რამენაირად მომეხერხებინა ტრანზაქციების დაჯგუფება. ვთქვათ ერთ ადამაინს თუ აქვს 10 ტრანზაქცია შესრულებული და მხოლოდ მეათეა isFraud=1, ჩვენ რომ შემთხვევით რენდომ სპლიტით 9 ტრეინში არ ჩავაგდოთ და ის 1 რეალური ინფრომაციის მატარებელი ტესტში არ ჩაგვივარდეს ამისთვის. ახლა დაჯგუფდა ტრანზაქციები რაღაც წინასწარშერჩეული ცვლადის მიხედვით და ერთ ჯგუფში არსებული ტრანზაქციები ან ყველა ტრეინში მოხვდება ან ყველა ტესტში. ასევე დავამატე LogisticRegression(class_weight='balanced'), მიუხედავად ამ ცვილებებისა, დატა ჯერ კიდევ დაუბალანსირებელია, რაც მეტრიკებზეც აისახება. ამიტო ამ არგუმენტის დამატებით მოდელი ცდილობს მეტი ყურადღება დაუთმოს უფრო იშვიათ შემთხვევებს. შედეგი მეტრიკებზეც აისახა, ROC auc იც გაიზარდა, არარეალური 0,96 მაჩვენებლიანი accuracy იც შემცირდა და f1 სქორიც და recall იც საკმაოდ გაიზარდა.

  

ასევე დაკვირვების პროცესში შევნიშნე რომ დატას სჭირდებოდა სტანდარტიზაცია, ამიტო დავამატე standard scaler და ისე დავატრენინგე მოდელი. ამას საკმაოდ კარგი შედეგი ჰქონდა მოდელზე. ROC auc 0,72 დან 0,82 მდე გაიზარდა, რაც საგრძნობი გაუმჯობესებაა, ასევე კარგი შედეგი ჰქონდა დანარჩენ მეტრიკებზეც.

  

ამის შემდეგ გადავწყვიტე Ordinal და OneHot ენკოდერების მაგივრად WOE ს გამოყენება, რადგან კლასიფიკაციის ამოცანაა და ამის გამოყენება კარგად შემეძლო. შედეგად ROC auc თითქმის არ შეცვლილა 0,01 ით დაიკლო, recall ში ძველმა მოდელმა აჯობა, f1 სქორი ახლა უკეთესი მივიღე, თუმცა ახლა როცა დატა ჯერ დაუბალანსირებელი მაქვს მაღალი recall და შესაბამისად ძველი მოდელი წესით უკეთესია, რადგან fraud ის ძებნისას უფრო მნიშვნელოვანია რეალური ფროდები არ გამორჩეს მოდელს.

  

ახლა გადავწყვიტე დატას დაბალნსირება. რადგან დაუბალანისრებელ დატაზე დატრენინგებულმა მოდელმა შეიძლება 99% accurancy დადოს მარა ეს არაფერს არ ნიშანვს ჩვენს შემთხვევაში რადგან fraud შემთხვევებს, რომლებიც რეალურ ცხოვრებაში უფრო იშვიათად ხდება, საკმარის ყურადღებას არ დაუთმობს. დაბალანსირების შემდეგ კი 50/50 ან 1:3 ზე პროპორციით გაყოფა წესით დაეხმარება მოდელს პატერნების უკეთესად დაჭერაში. დაჯოინების შემდეგ დატას გასპლიტვამდე ჩავამატე under sampling ის კოდი და class_weight='balanced' მოვაცილე ლოჯისტიკ რეგრეშენს. 50/50 ვარიანტიც ვცადე თუმცა 1:3 ზე გაყოფამ უკეთესი შედეგი მომცა, ასე გაყოფა ჩემი აზრით ისედაც უკეთესია რადგან შედარებით ვინარჩუნებთ პროპორციებს რაც რეალურ შემთხვევებში გვხვდებდა და არა პირდაპირ 50 50 ზე. 1/5 პროპორციაც ვცადე თუმცა ამან თითქმის არანირი განსხვავება არ მომცა. ძალიან გაიზარდა precision, რაც კარგია რადგან მოდელი როცა ამბობს რომ fraud ია, 80% პროცენტ შემთხვევაში მართალია და false positive ები არ ხდება. recall მცირედით გაიზარდა რაც მაინც კარგია რაგდან რეალური ფროდები მაინც პოვნადია და შესაბამისად f1 სქორიც საგრძნობლად გაიზარდა და წინა მოდელის 0,3 დან 0,6 მდე ავიდა. ROC auc იც გაიარდა 0,85 მდე.

  

საბოლოოდ, წესით ეს წერის პროცესში უნდა მექნა, თუმცა ცალცალკე ავტვირთე როგორც Data Cleaning ისე Data Engineering ის ნაწილები. რომლებსაც შემდეგ ცალცალკე ვალოუდებ და ერთ final_pipeline ში ვაერთიანებ სასურველ მოდელთან ერთად. Data Selection ნაწილს რაც შეეხება საბოლოოდ SHAP ით გავაანალიზე რომელ feature ს რა გავლენა აქვს, თუმცა ამ ექსპერიმენტში აღარ გადავწყივტე საჭიროდ სხვა რამეს გაკეთება, რა თქმა უნდა მოცემული დროის ფარგლებში, feature 18 და ნაწილობრივ feature 11 ის გაზრდა ტარგეტის ალბათობა 1 სკენ მიჰყავს, ხოლო feature 21, 127, 102 ის გაზრდას პირიქით, ტარგეტის ალბათობა 0 სკენ მიყავს. ამის გარდა შესაძლებელია კორელაციის მატრიცით ცვლადების გაცხრილვაც, ვცადე კიდეც წინა დავალების კოდი გამოვიყენე მარა ოდნავ უარესი შედეგი ჰქონდა და რანში აღარ მიჩვენებია, RFE იც შეიძლება თუმცა ბევრი სვეტი გვაქვს და წესით დიდ დროს მოანდომებს ეს საკმაოდ.

  

რაც შეეხება პარამეტრების ოპტიმზაციას წერის პროცესში, სანამ დატას ანდერსემპლინგს გავუკეთებდი ძალიან დიდი მონაცემები იყო და ამიტომ მიწევდა ყველა threshold ისთვის თითო-თითო ვარიანტის ხელით ცდა. თუმცა under sampling ის გაკეთების შემდეგ ნორმალური ზომის გახდა დატა, იმდენად დიდი დრო აღარ მიჰქონდა მოდელის დატრენინგებას და შესაბამისად რამდენიმე ვარიანტების ფორით გადაყოლა დავიწყე, რამაც უკეთესი პარამეტრებიც გამოაჩინა. ვინაიდან დატა დაპატარავდა თუ აქამდე one hot-ს 4 მნიშვნელობიან ფიჩერებზე ვზღუდავდი ახლა 5-მდე გავზარდე ლიმიტი და უკეთესადაც იმუშავა. ასევე გადავწყვიტე ამეწია nan ველიუების ზღვარი 0,8 დან 0,9-მდე, რადგან როგორც ვალიდაციის პროესმა აჩვენა იმ სხვაობაში შედარებით საჭირო ინფორმაცია იყრებოდა რომელსაც უკეთესი შედეგისკენ მიჰყავდა მოდელი. მცირედით გავზარდე Nuinque threshold -იც იმავე მიზეზით.

(დატა თავიდან ცოტა არასწორად მქონია გაყოფილი, ბოლოსკენ გავასწორე თუმცა მეტრიკებზე დიდი სხვაობა არ ქონია.)

  

რაც შეეხება overfit/underfit -ს საბოლოო მოდელმა საკმაოდ კარგი შედეგი დადო ამ მხრივ, 0,86 0,83 კარგი მოდელის მაჩვნეებელია წესით რადგან ეს არც მაღალ ბაიესს მიუთითებს და ROC auc -ის მცირედი განსხვავება ოვერფიტსაც გამორიცხავს.

  

Logistic Regression-ის საბოლოო მოდელი ავტვირთე კეგლზე და Score: 0.876448.

  
  

## **Decision Tree**

თუ სწორად გავიგე დავალებების წერის პროცესი, მომავალ მოდელებში კოდის წერა ნაკლები რაოდენობით მომიწევს, ასევე წესით ნაკლები სალაპარაკო მექნება, რადგან პრეპროცესინგის უდიდესი ნაწილი უკვე დაწერილი და გამზადებული მაქ. მხოლოდ მცირედი გაუმჯობესებების დამატება თუა შესაძლებელი. თუმცა ამ ნაწილში უფრო მნიშვნელოვანი ნაწილები იქნება threshold ების და ზოგადად საჭირო პარამეტრების სწორად შეფასება და განსაზღვრა.

  

პირველ რიგში, CustomCategoricalEncoderClass რასაც Logistic Regression ის შემთხვევაში ვიყენებდი ძალიან ჩახლართული იყო და რამე ხარვეზი რომ არ გაპარულიყო გავამარტივე. ახლა მხოლოდ ქვედა რეინჯის ენკოდერს იღებს კონსტრუქტორში, დანარჩენ ნაწილს კი engineering pipeline ის დანარჩენი ნაბიჯები აგვარებენ. ახლა გადავწყვიტე OneHot Ordinal ის მაგივრად OneHot Woe გამომეყენებინა. ვფიქრობ ორდინალ ენკოდერი ზედმეტად მარტივი ენკოდერია რადგან არანირაირ კავშირზე დაფუძნებით არ მოქმედებს და უბრალოდ 1 2 3 ს უსადაგებს კატეგორიული ცვლადის მნიშვნელობებს. WOE კი ტარგეტთან შესაბამისობაში ბევრად უკეთ მოქმედებს.

ასევე დავამატე გამზადებული pipeline ების mlflow ზე დალოგვა. ეს აქამდეც მქონდა თუმცა შედარებით ხავეზებით და წინა მოდელში მაინც არ ვიყენებდი, ტრეინინგის ნაწილში ახლიდან ვაწყობდი პაიპლაინს. ახლა კი უკვე ატვირთულ და გამზადებულ პაიპლაინს ვალოუდებ ტრენინგის პროცესში, ავაწყობ ერთ მთლიან ჩაჭვს და ისე ვასწავლი მოდელს. threshold ებს რა თქმა უნდა ვცვლი რომ სწორად მოვარგო მოდელს.

  

პირველ გაშვებაზე გადავწყვიტე ლოჯისტიკის ტრეინინგდან დარჩენილი საუკეთესო პარამეტრებზე გამეშვა decision tree-ს მოდელი. თვითონ decision tree-ს ჰიპერპარამეტრებს დაახლოებით გავუკეთე ინიციალიზაცია საწყისი მნიშვნელობებით. ტრეინის შემთხვევაში ROC auc 0,92 გამოვიდა ხოლო ვალიდაციის სეტზე 0,77. ეს საწყისი მოდელი აშკარად overfit-შია წასული რაც წესით მოსალოდნელი იყო საწყისი დაუოპტიმიზირებული ხისგან. იგივეს ამბობს f1 0,78-0,56 სქორები. ეს იმას ნიშნავს რომ ხე ზედმეტად დეტალურად სწავლობს სატრენინგო დატას და შემდეგ ვეღარ ახდენს დაზეპირებული და ტრეინზე მორებული პარამეტრებით ვალიდაციაზე კარგი შედეგის ჩვენებას. პირველ რიგში სანამ თვითონ decision tree -ს ჰიპერპარამეტრებს შევცვლიდი გადავწყვიტე ძველი პარამეტრების შეცვლა აღმოჩნდა რომ nunique_threshold, nan_threshol, onehot_threshold ის დაწევას უკეთესი შედეგი აქვს. 0,91 0,8 სასწაული გაუმჯობესება არ არის მაგრამ მაინც საგულისხმოა. რადგან threshold ების დაწევას უკეთესი შედეგი ჰქონდა და threshold ების ლისტებში ყველაზე დაბალი 0,8 მქონდა მითითებული, გადავწყვიტე კიდევ უფრო პატარა პარამეტრებზეც გამეშვა მოდელი. თუმცა ამას უარესი შედეგი ჰქონდა, როგორც ჩანს 0,8 0,8 4 threshold ები ოპტიმალურია ამ მომენტში. რომ დავუბრუნდეთ ჰიპერპარამეტრებს, ოვერფიტის პირველი მიზეზი წესით max_depth=10 არის რადგან საკმაოდ ღრმად ჩასვლა გამოსდის მოდელს. პირველ რიგში ამის უფრო დაბალი სიღრმეები უნდა მოვსინჯო. ჯერ 9 7 5 ვარიანტები ვცადე, და რადგან 9 ს ჰქონდა კარგი შედეგი შემდეგ 9 8 7 ვარიანტი ვცადე, შედეგად მივიღე რომ md=8 ყველაზე კარგი ვარიანტია, ეს აღარ დამილოგავს ჯერ 0,89 0,8 ROC დიდად გაუმჯობესება არაა და ამიტომ. ახლა min_samples_split რომ გადავიდეთ, ოვერფიტის მოსაშორებლად სიღრმისგან განსხვავებით საჭიროა გაზრდა რომ ნაკლები გაყოფა მოხდეს ნოუდების. 5 7 9 ვარიანტები მოვსინჯე და mss=7-ს ყველაზე კარგი შედეგი ჰქონდა, შემდეგ 6 7 8 ვცადე რომ ზუსტი ოპტიმაზაციის წერტილი მენახა და მაინც 7 ზე შეჩერდა მოდელი. ეს უკვე დავლოგე თუმცა 0,89 0,8 კვლავ ოვერფიტისკენ მიდის აშკარად. ასევე gini ის მაგივრად entropy იც ვცადე თუმცა როგორც ჩანს Decision Tree-ს გასაქანი აქ მთავრდება.

  
  

## **Random Forest**

სანამ მოდელს დავატრენინგებდეთ, იმედი უნდა გვქონდეს რომ რენდომ ფორესტი decision tree-ს აჯობებს, რადგან ოვერფიტის პრობლემას რომელსაც ხეებში ხშირად ვაწყდებით რენდომ ფორესტი საკმაოდ კარგად აგვარებს.

ამჯერადაც წინა დატოვებული მოდელის საუკეთესო პარამეტრები ავიღე და ახალი მოდელის შესაბამისი ჰიპერპარამეტრები დავუმატე დაახლოებით საწყისი მნიშვნელობებით. n_estimators = 50 70 100 რეინჯი ავიღე, საუკეთესო შედეგი 100-მა აჩვენა. როგორც მოსალოდნელი იყო Decision Tree-ს ოვერფიტი პირველივე დატრენინგებაზე მოგვარდა, Train Valid ზე შედეგები 0,91 0,85 აჩვენა რაც საკმაოდ კარგი ფიტია და ოვერფიტის ნიშნებიც გაქრა. შედარებით სხვაობაა f1 სქორებს შორის 0,68 0,54, უფრო თვალში საცემი სხვაობაა recall-ის შემთხვევაში 0,53 0,38, ვეცდები ეს მაჩვნეებლები დავაოპტიმიზირო.

შემდეგ გაშვებაზე რადგან 50 70 100 დან ყვეალზე დიდი აირჩა რეინჯი გადავწიე და 100 200 500 ავიღე. საუკეთესო შედეგი 500 მა აჩვენა თუმცა იმდენად მცირე განსხვავება იყო რომ n_estimator ის ამხელა გაზრდა არ ჩავთვალე სწორად და ამიტომ საბოლოოდ 200 ზე შევჩერდი.

გადავწყვიტე ისევ წინა მოდელისგან შემორჩენილ პარამეტრებს დავუბურნდე და მოდელის ჰიპერპარამეტრებთან ერთად მოვძებნო საუკეთესო ვარიანტი. ახლა გადავწყვიტე ამ მომენტის საუკეთესო პარაემტრების ახლო მიდამოში ველიუები ამეღო და გრიდ სერჩი გამეშვა, ასე მცირე გაუმჯოებსებებიც უფრო რთულად გამორჩენადია, ისედაც წინა მოდელზე მორგებული პარამეტრების მიხედვით მიყოლას ჯობდა რომ ამ მოდელისთვის მეცადა საუკეთესო ველიუების პოვნა. შედეგად მივიღე ისეთი პარამეტრები რომლებმაც recall საკმაოდ გაზარდეს წინა პარაემტრებთან შედარებით, ასევე მოიმატა f1 სქორმაც. თუმცა ROC 0,94 0,87 წინასთან შედარებით გაუმჯოებსებულია მაგრამ მაინც ოვერფიტისკენ მიყავს მოდელი და ეს არ მომწონს.

დავამტე min_samples_leaf ჰიპერპარამეტი რენდომ ფორესტ მოდელს რომ ნაკლებად გაიქცეს ოვერფიტისკენ. ასევე impurity ს შესამოწმებლად დავბრუნდი gini ზე. ასევე დავწიე max_depth ცვლადის მნიშვნელობა დაბლა, ასე ROC auc ცოტათი შემცირდა, სამაგიეროდ მოდელი ნაკლებად მიდის ოვერფიტში შედარებით და f1 სქორებიც გაუმჯობესდა.

შემდეგ გაშვებაზე დამატებით feature selection-სთვის დავამატე CustomCorrHandlerClass კლასი, რომელიც ცვლადებს კორელაციის მიხედვით გადაყრის და მოდელს შეუმსუბუქებს დატას. რამდენიმე threshold მოვსინჯე და საუკეთესო 0,9 აღმოჩნდა, როგორც ჩანს მასზე ქვემოთ უკვე მნიშვნელოვანი ინფორმაციის მატარებელი feature ებიც იყრება. დიდად უკეთესი შედეგი არ მოუცია, recall ში ოდანვ გაუარესდა კიდეც თუმცა მინდოდა ესეც გამომეყენებინა.

საბოლოოდ class_weight='balanced' დავამატე მოდელს როგორც ამას ლოჯისტიკის შემთხვევაში ვიყენებდი თავიდან, რადგან სტატისტიკას რო დავაკვირდი აშკარად precision საკმაოდ მაღალი იყო ყველა მოდელზე recall კი პირიქით 0,5-საც ვერ აცდა, ანუ ნაწილობრივ კიდე დასაბალანსირებელია ჩემი დატა, ახლა კი ROC auc მცირედით გაზარდა თუმცა ძალიან კარგი შედეგი ჰქონდა recall-ზე და შესაბამისად f1 score იც გაიზარდა. მართალია precision -მა შედარებით დაიკლო თუმცა fraud detection -ში წესით recall უფრო გვაინტერესებს რომ რეალური ფროდები არ გაგვეპაროს.

  
  

## **AdaBoost**

ამ მოდელზე დიდი ხანი არ გავჩერდები, XGBoost უნდა დავატრენინგო მაინც და ამ დროითი რესურსის ფარგლებში მირჩევნია იმას დავუთმო მეტი დრო. რენდომ ფორესტის შემდეგ გადავწყივტე adaboost მოდელი დამეტრენინგებინა ჩემს დატაზე. პირველ გაშვებაზე ხის მახასიათებელი პარამეტრები ამოვიღე და learning rate ჩავამატე default მნიშვნელობით, დანარჩენ პარამეტრებზე წინა მოდელისგან შემორჩენილი ველიუები გამოვიყენე. ROC auc 0,9 0,81, f1 სქორი 0,69 0,44 პირველი გაშვება overfit-სკენ წავიდა და ვეცდები გავაუმჯობესო. ამ მომენტში n_estimator=100 და learning rate default მნიშვენლობა რაც 1.0 ია ნიშნავს რომ მოდელი აიღებს 100 ცალ მოდელს და უფრო ხისტად გააკეთებს განახლებებს. ამიტომ ჯობია რომ ნელნელა learning rate დავწიო დაბლა და პირიქით n_estimators გავზარდო რომ ბევრი მოდელის ფონზე ნელნელა და სწორად ისწავლოს ადაბუსტმა. პირდაპირ მარტო n_estimators რომ გმაეზარდა წესით უფრო მეტად მიაქცევდა noise ს ყურადღებას და მეტი ოვერფიტი იქნებოდა. ამიტომ შემდეგ გაშვებაზე ვცადე 0,5 0,1 threshold ები learning rate სთვის და 400 600 n_estimator ისთვის. უკეთესი შედეგი 0,5 600 წყვილმა აჩვენა ROC f1 მაჩვნეებლები გაიზარდა თუმცა მოდელი მაინც ოვერფიტისკენ მიდის, შემდეგ ვცადე 0,05 800, ოვერფიტი გაუმჯობესდა 0,9 0,84 ROC უკვე ნორმალური მაჩვენებელია. თუმცა f1 სქორი გაუარესდა. recall 0.24 საკმაოდ ცუდი შედეგია. ამ მიმართულებით ბოლოს 0,01 1000 წყვილი ვცადე, რამააც გააუარესა შედეგი. როგორც ჩანს აქეთ წამოსვლა უკვე ხელს უშლის მოდელს, ზედმეტად ნელა და ბევრ მოდელზე სწავლობს, რაც საერთო შედეგს აგდებს. საუკეთესო ვარიანტი 0,5 600 წყვილი აღმოჩნდა, უფრო ახლო რეინჯის მიდამოებშიც შეიძლება ძებნა 0,4 0,5 0,6 500 600 700 ვარიანტებით, თუმცა ეს ოდნავ გააუმჯობესებს მოდელს მხოლოდ მეტი დროის შემთხვევაში ღირს ცდად.

  

## **XGBoost**

საბოლოოდ გამოვიყენებ XGBoost-ს და ვეცდები მაქსიმალურად კარგად მოვარგო ეს მოდელი დავალებას. პირველ გაშვებაზე ადაბუსტის დატოვებული პარამეტრები გამოვიყენე და max_depth=5 ჰიპერპარამეტრი დავამატე. ვალიდაციაზე ეგრევე აჯობა ადაბუსტს 0,87 მნიშვნელობით, თუმცა ეს მოდელი ჯერჯერობით ძალიან მიდის ოვერფიტში ამას ROC auc-ც აჩვენებს და f1 სქორიც.

ამის შემდეგ threshold-ებზე უბრალოდ for loop-ებით გადაყოლის მაგივრად grid search ავაწყე, თან cross-validation-ს გამოვიყენებ StratifiedKFold კფოლდით რომ თუ კიდევ დასაბალნსირებელია დატა უკეთ გაანაწილოს გაყოფიბის მომენტში. ასევე გადავცვალე n_estimators და learning rate adaboost-ის საუკეთესო მაჩვენებლებზე 600 0,5. შედეგად მივიღე ყველაზე დიდი ოვერფიტის მქონე მოდელი. correalation matrix -ით ფიჩერების გადაყრაც ამოვიღე სხვანაირად ვაპირებ რომ მოვაგვარო მომავალში.(logging-ში დამრჩენია უბრალოდ რიცხვის ატვირთვა და ამიტომ მაინც ჩანს).

წინა მოდელის ჰიპერპარამეტრები კარგად არ ერგება ამ მოდელს, შესაბამისად ვიწყებ მათ გასწორებას. პირველ რიგში n_estimators დავწიე 150 მდე, ხოლო max_depth 3 მდე რომ შემემცირებინა ოვერფიტი. ამას აშკარა დადებითი შედეგი ჰქონდა მოდელზე რადგან ტრეინზე ROC auc -მ დაიკლო და ბოლომდე აღარ იზეპირებს მოდელი ტრეინ დატას და ვალიდაციის შედეგმა მოიმატა 0,87 მდე. იგივე დაეტყო სხვა მეტრიკებსაც. თუმცა ჯერ კიდევ არ ვარგა მოდელი და ძალიან იმახსოვრებს ტრეინ დატას.

შემდეგ კიდევ მოვძებნე 100 150 რეინჯი n_estimator სთვის, ხოლო learning rate კიდევ უფრო დავწიე 0,01 0,005 მდე, ასევე max_depth ჩავსვი 3 4 შუალედში. შედეგად მივიღე კიდევ უფრო ნაკლებად ოვერფიტში წასული მოდელი. ROC auc 0,9 0,83 წინა მაჩვენებლბებს წესით ჯობია ოვერფიტის მხრივ, თუმცა კიდევ სჭირდება გაუმსჯობესება, f1 სქორიც დავარდა recall თან ერთად.

საბოლოოდ n_estimator 150, learning rate 0,05 ვფიქრობ კარგი ROC auc აჩვენა და ამ მომენტში ამ threshold-ზე შევჩერდები. f1 სქორის გამოსწორება შემიძლია y_valid_pred ის 0,5 ის მაგივრად სხვა threshold ზე შეხდვით დაგენრერირებით, 0,3 ვცადე აღარ დამილოგავს და რა თქმა უნდა გაუმჯობესდა recall და შესაბამისად f1 სქორიც, თუმცა ჯერ სხვა რაღაცეებსაც დავამატებ.

პრეპროცესინგის ნაწილს რომ დავუბრუნდეთ, ლოჯისტიკ რეგრეშენის დროს კარგად მოვაგვარე ეს საკითხი თუმცა შესაძლოა კონკრეტულად XGBoost-სთვის უკეთესი შედეგების მიღება შევძლო ცვლილებებით. imputing და scaling ის ნაწილები დავაიგნორე ჩამოლოუდებულ პაიპლაინში, რადგან ამას მოდელი თავისითაც მოაგვარებს.

ასევე ვცადე 3:1 პროპორციების მაგივრად 5:1 under sampling ის პროცესში, თუმცა ამას ცუდი გავლენა ჰქონდა recall ზე და შესაბამისად f1 სქორიც დავარდა.

ამის შემდეგ გადავწყივტე ამოღებული კორელაციის მატრიცის გამცხრილავის მაგივრად SHAP -ის დახმარებით მომეგვარებინა feature selection. ვნახე რომელ feature-ს როგორი გავლენა აქვს საბოლოო ტარგეტზე.(სურათი notebook-ში ჩანს mlflow-ზე ვერ ავტვირთე კარგად რატომღაც), ამის შემდეგ გამოჩნდა რომ ტოპ რამდენიმე feature არის ისეთი რომელსაც ძალიან დიდი გავლენა აქვს ტარგეტზე და დანარჩენი ცვლადები უბრალოდ ტვირთავენ მოდელს. ამიტომ ტოპ 10 მნიშვნელოვანი ცვლადი ამოვიღე და დავამატე CustomSHAPSelectorClass რომელიც ამ სვეტებს დატოვებს და დანარჩენებს გადაყრის. აქ ტრეინის კოდის გამეორება მომიწია კარგად ვერ მოვიფიქრე როგორ არ მომწეოდა გადმოკოპირება, მაგრამ მთავარი ისაა რომ ახლა საბოლოო პაიპლაინში ეს engineering -ის შემდეგ ეს CustomSHAPSelectorClass კლასციაა ჩამატებული და მოდელთან გაცხრირული დატა მიდის მხოლოდ. აქ გამოჩნდა რომ key_id ცვლადს რომელიც დატა მერჯის დროს დავაგენერირე დაახლოებით სამჯერ უფრო მნიშვნელოვანია ვიდრე მეორე ყველაზე მნიშვნელოვანი ცვლადი, შესაბამისად ამ ცვლადზე მეტი ყურადღებაა მისაქცევი. საბოლოოდ შაპის გამოყენებით მოდელმა საუკეთესო რანზე ოდნავ ცუდი შედეგი დადო თუმცა მეტი დროის შემთხვევაში ღირს დასახვეწად.

საბოლოოდ პრედიქტის დროს 0,5 threshold-ს for ით გადავუყევი და საუკეთესო ვარიანტი 0,2 0,3 ს შორის დაჯდა, ამიტომ 0,25 ეს მნიშვნელობა. 0,93 0,86 ROC auc ერთერთი საუკეთსო შედეგია, recall იც ძალიან გაიზრდა და შესაბამისად მიყვა f1 სქორიც და იმხელა განსხვავება აღარაა. ამის მერე პრეპროცესინგის მერჯის ნაწილში მინდოდა სხვა ვარიანტებიც მეცადა, key_id გენერაციის დროს DeviceType იც დავამატე, ამან მცირედ თუმცა მაინც გააუმჯობესა მოდელი.

  

XGBoost-ის საბოლოო მოდელი ავტვირთე კეგლზე და Score: 0.901281.

  
  

## MLFLOW Tracking:

ექსპერიმენტების ბმული: https://dagshub.com/gkuch22/ml-hw2-gkuch22.mlflow

საუკეთესო მოდელის შედეგი: Private score: 0.881607, Score: 0.901281.
